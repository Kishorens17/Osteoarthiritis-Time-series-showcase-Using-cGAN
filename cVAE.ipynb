{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa3db839-be6c-467a-8d5b-059646dd4ead",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Successfully loaded 5778 images.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# ===================================================================\n",
    "# 1. Configuration\n",
    "# ===================================================================\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Paths\n",
    "DATASET_PATH = \"C:/Users/Sanjjey Arumugam/.cache/kagglehub/datasets/jeftaadriel/osteoarthritis-initiative-oai-dataset/versions/1/train\"\n",
    "OUTPUT_DIR = \"cvae_generated_images\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 30\n",
    "LEARNING_RATE = 1e-4\n",
    "IMAGE_SIZE = 64\n",
    "IMAGE_CHANNELS = 1\n",
    "LATENT_DIM = 100\n",
    "NUM_CLASSES = 5\n",
    "\n",
    "# ===================================================================\n",
    "# 2. Dataset\n",
    "# ===================================================================\n",
    "class OaiDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        for i in range(NUM_CLASSES):\n",
    "            class_dir = os.path.join(self.root_dir, str(i))\n",
    "            if os.path.isdir(class_dir):\n",
    "                paths = glob.glob(os.path.join(class_dir, '*.png'))\n",
    "                self.image_paths.extend(paths)\n",
    "                self.labels.extend([i] * len(paths))\n",
    "            else:\n",
    "                print(f\"Warning: Directory not found for class {i}: {class_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"L\")\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.CenterCrop(IMAGE_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    print(f\"ERROR: Dataset path not found: {DATASET_PATH}\")\n",
    "else:\n",
    "    dataset = OaiDataset(root_dir=DATASET_PATH, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    print(f\"Successfully loaded {len(dataset)} images.\")\n",
    "\n",
    "# ===================================================================\n",
    "# 3. CVAE Model\n",
    "# ===================================================================\n",
    "class CVAE(nn.Module):\n",
    "    def __init__(self, latent_dim=LATENT_DIM, num_classes=NUM_CLASSES, img_channels=IMAGE_CHANNELS):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.img_channels = img_channels\n",
    "\n",
    "        # Embed labels into latent_dim space\n",
    "        self.embedding = nn.Embedding(num_classes, latent_dim)\n",
    "\n",
    "        # ===== Encoder =====\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(img_channels + 1, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(256 * 4 * 4, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256 * 4 * 4, latent_dim)\n",
    "\n",
    "        # ===== Decoder =====\n",
    "        self.decoder_input = nn.Linear(latent_dim + latent_dim, 256 * 4 * 4)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, img_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def encode(self, x, y):\n",
    "        y_embed = self.embedding(y).unsqueeze(2).unsqueeze(3)\n",
    "        y_map = y_embed.mean(1, keepdim=True).expand(-1, 1, x.size(2), x.size(3))\n",
    "        combined = torch.cat([x, y_map], dim=1)\n",
    "        result = self.encoder(combined)\n",
    "        mu = self.fc_mu(result)\n",
    "        logvar = self.fc_logvar(result)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, y):\n",
    "        y_embed = self.embedding(y)\n",
    "        combined = torch.cat([z, y_embed], dim=1)\n",
    "        result = self.decoder_input(combined)\n",
    "        result = result.view(-1, 256, 4, 4)\n",
    "        return self.decoder(result)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        mu, logvar = self.encode(x, y)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        reconstruction = self.decode(z, y)\n",
    "        return reconstruction, mu, logvar\n",
    "\n",
    "# ===================================================================\n",
    "# 4. Loss\n",
    "# ===================================================================\n",
    "def vae_loss_function(recon_x, x, mu, logvar):\n",
    "    recon_loss = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
    "    kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + kl_div\n",
    "\n",
    "# ===================================================================\n",
    "# 5. Training\n",
    "# ===================================================================\n",
    "model = CVAE().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# print(\"Starting C-VAE Training...\")\n",
    "# for epoch in range(NUM_EPOCHS):\n",
    "#     total_loss = 0\n",
    "#     for batch_idx, (real_images, labels) in enumerate(dataloader):\n",
    "#         real_images = real_images.to(DEVICE)\n",
    "#         labels = labels.to(DEVICE)\n",
    "\n",
    "#         recon_images, mu, logvar = model(real_images, labels)\n",
    "#         loss = vae_loss_function(recon_images, real_images, mu, logvar)\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         if (batch_idx + 1) % 100 == 0:\n",
    "#             print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Batch [{batch_idx+1}/{len(dataloader)}], Loss: {loss.item()/len(real_images):.4f}\")\n",
    "\n",
    "#     print(f\"====> Epoch: {epoch+1} Average loss: {total_loss / len(dataset):.4f}\")\n",
    "\n",
    "#     if (epoch + 1) % 10 == 0:\n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             num_samples = NUM_CLASSES\n",
    "#             fixed_noise = torch.randn(num_samples, LATENT_DIM).to(DEVICE)\n",
    "#             fixed_labels = torch.arange(0, NUM_CLASSES).to(DEVICE)\n",
    "#             generated_images = model.decode(fixed_noise, fixed_labels)\n",
    "#             save_image(generated_images.view(num_samples, 1, IMAGE_SIZE, IMAGE_SIZE),\n",
    "#                        os.path.join(OUTPUT_DIR, f'sample_epoch_{epoch+1}.png'),\n",
    "#                        nrow=NUM_CLASSES, normalize=True)\n",
    "#         model.train()\n",
    "\n",
    "# print(\"âœ… Training Finished.\")\n",
    "\n",
    "# ===================================================================\n",
    "# 6. Final Generation\n",
    "# ===================================================================\n",
    "# print(\"Generating final images for each stage...\")\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     for stage in range(NUM_CLASSES):\n",
    "#         num_final_samples = 16\n",
    "#         noise = torch.randn(num_final_samples, LATENT_DIM).to(DEVICE)\n",
    "#         labels = torch.full((num_final_samples,), stage, dtype=torch.long).to(DEVICE)\n",
    "#         final_images = model.decode(noise, labels)\n",
    "#         save_image(final_images.view(num_final_samples, 1, IMAGE_SIZE, IMAGE_SIZE),\n",
    "#                    os.path.join(OUTPUT_DIR, f'final_stage_{stage}.png'),\n",
    "#                    nrow=4, normalize=True)\n",
    "#         print(f\"âœ… Saved final images for stage {stage}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67f7196a-447d-4995-8ff2-42e61d38f38c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m MODEL_PATH \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(OUTPUT_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcvae_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m torch\u001b[38;5;241m.\u001b[39msave({\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m: NUM_EPOCHS,\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m: model\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m: optimizer\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[0;32m      8\u001b[0m }, MODEL_PATH)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Model saved at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'loss' is not defined"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = os.path.join(OUTPUT_DIR, \"cvae_model.pth\")\n",
    "\n",
    "torch.save({\n",
    "    \"epoch\": NUM_EPOCHS,\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    \"loss\": loss.item(),\n",
    "}, MODEL_PATH)\n",
    "\n",
    "print(f\"âœ… Model saved at {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "802915a8-eb1a-4161-94f4-c4b982a8c959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchmetrics in c:\\users\\sanjjey arumugam\\anaconda3\\lib\\site-packages (1.8.1)\n",
      "Collecting pytorch-fid\n",
      "  Downloading pytorch_fid-0.3.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: numpy>1.20.0 in c:\\users\\sanjjey arumugam\\anaconda3\\lib\\site-packages (from torchmetrics) (2.1.3)\n",
      "Requirement already satisfied: packaging>17.1 in c:\\users\\sanjjey arumugam\\anaconda3\\lib\\site-packages (from torchmetrics) (24.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\sanjjey arumugam\\anaconda3\\lib\\site-packages (from torchmetrics) (2.8.0+cu129)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in c:\\users\\sanjjey arumugam\\anaconda3\\lib\\site-packages (from torchmetrics) (0.15.2)\n",
      "Requirement already satisfied: pillow in c:\\users\\sanjjey arumugam\\anaconda3\\lib\\site-packages (from pytorch-fid) (11.1.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\sanjjey arumugam\\anaconda3\\lib\\site-packages (from pytorch-fid) (1.15.3)\n",
      "Requirement already satisfied: torchvision>=0.2.2 in c:\\users\\sanjjey arumugam\\anaconda3\\lib\\site-packages (from pytorch-fid) (0.23.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sanjjey arumugam\\anaconda3\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (72.1.0)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\sanjjey arumugam\\anaconda3\\lib\\site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\sanjjey arumugam\\anaconda3\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (3.17.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\sanjjey arumugam\\anaconda3\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\sanjjey arumugam\\anaconda3\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sanjjey arumugam\\anaconda3\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\sanjjey arumugam\\anaconda3\\lib\\site-packages (from torch>=2.0.0->torchmetrics) (2025.3.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sanjjey arumugam\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sanjjey arumugam\\anaconda3\\lib\\site-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n",
      "Downloading pytorch_fid-0.3.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: pytorch-fid\n",
      "Successfully installed pytorch-fid-0.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchmetrics pytorch-fid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdb2cef0-9d12-4f26-a978-980b5a9de969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ssim\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_mse, total_ssim, count = 0, 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            recon_images, mu, logvar = model(images, labels)\n",
    "\n",
    "            # --- MSE ---\n",
    "            mse = nn.functional.mse_loss(recon_images, images, reduction=\"mean\").item()\n",
    "            total_mse += mse * images.size(0)\n",
    "\n",
    "            # --- SSIM (convert to numpy & denormalize to [0,1]) ---\n",
    "            for i in range(images.size(0)):\n",
    "                orig = images[i].cpu().numpy().squeeze()\n",
    "                recon = recon_images[i].cpu().numpy().squeeze()\n",
    "                orig = (orig + 1) / 2.0   # denormalize from [-1,1] â†’ [0,1]\n",
    "                recon = (recon + 1) / 2.0\n",
    "                total_ssim += ssim(orig, recon, data_range=1.0)\n",
    "                count += 1\n",
    "\n",
    "    avg_mse = total_mse / count\n",
    "    avg_ssim = total_ssim / count\n",
    "    print(f\"ðŸ”Ž Evaluation Results: MSE={avg_mse:.4f}, SSIM={avg_ssim:.4f}\")\n",
    "    return avg_mse, avg_ssim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb1af382-dfbb-424f-9524-bf72b2f33ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model loaded. Resuming from epoch 100\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "\n",
    "model = CVAE().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "start_epoch = checkpoint[\"epoch\"]\n",
    "print(f\"âœ… Model loaded. Resuming from epoch {start_epoch}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbf4e026-dda8-48d8-977f-8463df9adb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”Ž Evaluation Results: MSE=0.0050, SSIM=0.8559\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.005026210344297564, np.float64(0.8559373979817012))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(model, dataloader, DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af9ac144-b33f-47fb-b276-37d550681b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-msssim\n",
      "  Downloading pytorch_msssim-1.0.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\sanjjey arumugam\\anaconda3\\lib\\site-packages (from pytorch-msssim) (2.8.0+cu129)\n",
      "Requirement already satisfied: filelock in c:\\users\\sanjjey arumugam\\anaconda3\\lib\\site-packages (from torch->pytorch-msssim) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\sanjjey arumugam\\anaconda3\\lib\\site-packages (from torch->pytorch-msssim) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\sanjjey arumugam\\anaconda3\\lib\\site-packages (from torch->pytorch-msssim) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\sanjjey arumugam\\anaconda3\\lib\\site-packages (from torch->pytorch-msssim) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sanjjey arumugam\\anaconda3\\lib\\site-packages (from torch->pytorch-msssim) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\sanjjey arumugam\\anaconda3\\lib\\site-packages (from torch->pytorch-msssim) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sanjjey arumugam\\anaconda3\\lib\\site-packages (from torch->pytorch-msssim) (72.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sanjjey arumugam\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch->pytorch-msssim) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sanjjey arumugam\\anaconda3\\lib\\site-packages (from jinja2->torch->pytorch-msssim) (3.0.2)\n",
      "Downloading pytorch_msssim-1.0.0-py3-none-any.whl (7.7 kB)\n",
      "Installing collected packages: pytorch-msssim\n",
      "Successfully installed pytorch-msssim-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-msssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d8c771d-1dc9-46ed-b4cc-85c6867caf6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Evaluation Results: MSE=0.1603, SSIM=0.3317\n",
      "Final Evaluation -> MSE: 0.1603, SSIM: 0.3317\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from pytorch_msssim import ssim\n",
    "\n",
    "# =========================\n",
    "# Dataset\n",
    "# =========================\n",
    "class OAIDataset(Dataset):\n",
    "    def __init__(self, root, split=\"train\", image_size=128):\n",
    "        self.root = Path(root) / split\n",
    "        self.samples = []\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),  # force 128x128\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5])  # normalize to [-1, 1]\n",
    "        ])\n",
    "        for stage_folder in sorted(self.root.iterdir()):\n",
    "            stage = int(stage_folder.name)  # 0..4\n",
    "            for img_path in stage_folder.glob(\"*.png\"):\n",
    "                side = 0\n",
    "                if \"_2.png\" in img_path.name:\n",
    "                    side = 1\n",
    "                self.samples.append((img_path, stage, side))\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, stage, side = self.samples[idx]\n",
    "        img = Image.open(img_path).convert(\"L\")\n",
    "        img = self.transform(img)\n",
    "        return img, torch.tensor(stage), torch.tensor(side)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Evaluation function\n",
    "# =========================\n",
    "def evaluate_cgan_model(model, dataloader, device, latent_dim=128):\n",
    "    model.eval()\n",
    "    mse_loss = nn.MSELoss()\n",
    "    total_mse, total_ssim, count = 0.0, 0.0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for real_images, stage, side in dataloader:\n",
    "            real_images = real_images.to(device)\n",
    "            stage = stage.to(device)\n",
    "            side = side.to(device)\n",
    "\n",
    "            # Random latent vector (correct shape)\n",
    "            z = torch.randn(real_images.size(0), latent_dim, device=device)\n",
    "\n",
    "            # Generate images\n",
    "            fake_images = model(z, stage, side)\n",
    "\n",
    "            # Compute metrics\n",
    "            mse_val = mse_loss(fake_images, real_images).item()\n",
    "            ssim_val = ssim(\n",
    "                fake_images.cpu(), real_images.cpu(),\n",
    "                data_range=2.0, size_average=True\n",
    "            ).item()\n",
    "\n",
    "            total_mse += mse_val\n",
    "            total_ssim += ssim_val\n",
    "            count += 1\n",
    "\n",
    "    avg_mse = total_mse / count\n",
    "    avg_ssim = total_ssim / count\n",
    "    print(f\"âœ… Evaluation Results: MSE={avg_mse:.4f}, SSIM={avg_ssim:.4f}\")\n",
    "    return avg_mse, avg_ssim\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Load checkpoint and model\n",
    "# =========================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Update path to your checkpoint\n",
    "checkpoint = torch.load(r\"C:\\Users\\Sanjjey Arumugam\\results\\checkpoint_epoch_200.pth\", map_location=device)\n",
    "\n",
    "# Make sure these match the original Generator definition\n",
    "netG = Generator(nz=128, n_stage=5, n_side=2, image_size=128).to(device)\n",
    "netG.load_state_dict(checkpoint['netG_state_dict'])\n",
    "\n",
    "# =========================\n",
    "# Dataset & Dataloader\n",
    "# =========================\n",
    "dataset = OAIDataset(root=\"C:/Users/Sanjjey Arumugam/.cache/kagglehub/datasets/jeftaadriel/osteoarthritis-initiative-oai-dataset/versions/1\", \n",
    "                     split=\"train\", image_size=128)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# =========================\n",
    "# Evaluate\n",
    "# =========================\n",
    "mse, ssim_score = evaluate_cgan_model(netG, dataloader, device, latent_dim=128)\n",
    "print(f\"Final Evaluation -> MSE: {mse:.4f}, SSIM: {ssim_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23825cb6-d7cf-4f71-af6d-b741a75f9909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Generating images for FID evaluation...\n",
      "ðŸš€ Computing FID per stage...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [00:11<00:00,  3.07it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import os\n",
    "from pytorch_fid import fid_score\n",
    "\n",
    "# =========================\n",
    "# Dataset\n",
    "# =========================\n",
    "class OAIDataset(Dataset):\n",
    "    def __init__(self, root, split=\"train\", image_size=128):\n",
    "        self.root = Path(root) / split\n",
    "        self.samples = []\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5])  # [-1,1]\n",
    "        ])\n",
    "        for stage_folder in sorted(self.root.iterdir()):\n",
    "            stage = int(stage_folder.name)  # 0..4\n",
    "            for img_path in stage_folder.glob(\"*.png\"):\n",
    "                side = 0\n",
    "                if \"_2.png\" in img_path.name:\n",
    "                    side = 1\n",
    "                self.samples.append((img_path, stage, side))\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, stage, side = self.samples[idx]\n",
    "        img = Image.open(img_path).convert(\"L\")\n",
    "        img = self.transform(img)\n",
    "        return img, torch.tensor(stage), torch.tensor(side)\n",
    "\n",
    "# =========================\n",
    "# GAN Generator\n",
    "# =========================\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, nz=128, ngf=64, nc=1, n_stage=5, n_side=2, image_size=128):\n",
    "        super().__init__()\n",
    "        self.stage_emb = nn.Embedding(n_stage, n_stage)\n",
    "        self.side_emb  = nn.Embedding(n_side, n_side)\n",
    "        self.init_size = image_size // 16\n",
    "        cond_dim = n_stage + n_side\n",
    "        self.fc = nn.Linear(nz + cond_dim, ngf*8*self.init_size*self.init_size)\n",
    "\n",
    "        def block(in_feat, out_feat):\n",
    "            return nn.Sequential(\n",
    "                nn.Upsample(scale_factor=2),\n",
    "                nn.Conv2d(in_feat, out_feat, 3, padding=1),\n",
    "                nn.BatchNorm2d(out_feat),\n",
    "                nn.ReLU(True)\n",
    "            )\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            block(ngf*8, ngf*4),\n",
    "            block(ngf*4, ngf*2),\n",
    "            block(ngf*2, ngf),\n",
    "            block(ngf, ngf//2),\n",
    "            nn.Conv2d(ngf//2, nc, 3, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z, stage, side):\n",
    "        cond = torch.cat([self.stage_emb(stage), self.side_emb(side)], dim=1)\n",
    "        x = torch.cat([z, cond], dim=1)\n",
    "        out = self.fc(x).view(x.size(0), -1, self.init_size, self.init_size)\n",
    "        return self.conv_blocks(out)\n",
    "\n",
    "# =========================\n",
    "# Paths and Device\n",
    "# =========================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "checkpoint_path = r\"C:\\Users\\Sanjjey Arumugam\\results\\checkpoint_epoch_200.pth\"\n",
    "dataset_root = \"C:/Users/Sanjjey Arumugam/.cache/kagglehub/datasets/jeftaadriel/osteoarthritis-initiative-oai-dataset/versions/1\"\n",
    "\n",
    "generated_root = Path(\"cgan_generated\")\n",
    "generated_root.mkdir(exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# Load checkpoint\n",
    "# =========================\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "netG = Generator(nz=128, n_stage=5, n_side=2, image_size=128).to(device)\n",
    "netG.load_state_dict(checkpoint['netG_state_dict'])\n",
    "netG.eval()\n",
    "\n",
    "# =========================\n",
    "# Dataset & Dataloader\n",
    "# =========================\n",
    "dataset = OAIDataset(root=dataset_root, split=\"train\", image_size=128)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "latent_dim = 128\n",
    "\n",
    "# =========================\n",
    "# Generate images per stage\n",
    "# =========================\n",
    "print(\"ðŸš€ Generating images for FID evaluation...\")\n",
    "stage_dirs = []\n",
    "with torch.no_grad():\n",
    "    for stage in range(5):\n",
    "        stage_dir = generated_root / f\"stage_{stage}\"\n",
    "        stage_dir.mkdir(exist_ok=True)\n",
    "        stage_dirs.append(stage_dir)\n",
    "\n",
    "        # Filter dataset for this stage\n",
    "        stage_samples = [(img, s, side) for img, s, side in dataset.samples if s == stage]\n",
    "        for i in range(0, len(stage_samples), 64):\n",
    "            batch_samples = stage_samples[i:i+64]\n",
    "            imgs = []\n",
    "            stages = []\n",
    "            sides = []\n",
    "            for img_path, s, side in batch_samples:\n",
    "                img = Image.open(img_path).convert(\"L\")\n",
    "                img = transforms.Compose([\n",
    "                    transforms.Resize((128,128)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize([0.5],[0.5])\n",
    "                ])(img)\n",
    "                imgs.append(img)\n",
    "                stages.append(torch.tensor(s))\n",
    "                sides.append(torch.tensor(side))\n",
    "            imgs = torch.stack(imgs).to(device)\n",
    "            stages = torch.stack(stages).to(device)\n",
    "            sides = torch.stack(sides).to(device)\n",
    "            \n",
    "            z = torch.randn(imgs.size(0), latent_dim, device=device)\n",
    "            fake_imgs = netG(z, stages, sides)\n",
    "            for j in range(fake_imgs.size(0)):\n",
    "                save_image(fake_imgs[j], stage_dir / f\"img_{i+j}.png\", normalize=True)\n",
    "\n",
    "# =========================\n",
    "# Compute FID per stage\n",
    "# =========================\n",
    "print(\"ðŸš€ Computing FID per stage...\")\n",
    "for stage in range(5):\n",
    "    real_stage_dir = Path(dataset_root) / \"train\" / str(stage)\n",
    "    fake_stage_dir = generated_root / f\"stage_{stage}\"\n",
    "    fid_val = fid_score.calculate_fid_given_paths([str(real_stage_dir), str(fake_stage_dir)],\n",
    "                                                  batch_size=64,\n",
    "                                                  device=device,\n",
    "                                                  dims=2048)\n",
    "    print(f\"Stage {stage} -> FID: {fid_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44007aca-05cb-4287-b4cd-72de303f26d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
